{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a28oxR8ZUIRj"
      },
      "source": [
        "# Solving Coin Flip Game using Monte Carlo\n",
        "\n",
        "## Game Description\n",
        "\n",
        "A player has three turns.  During each turn they can flip a coin or pass.  They accumulate money in their pot during each turn.  At the end of three turns, they get to keep the money in the pot.  \n",
        "\n",
        "- If they \"pass\" during a turn, then the game ends and they keep the money in the pot.   \n",
        "- If they \"flip\" during a turn, then a random 50/50 coin is flipped.  If it lands heads, then 1 dollar is added to the pot.  If it lands tails, then all the money is removed from the pot.  \n",
        "\n",
        "\n",
        "Our goal is to figure out how to flip/pass at each turn to maximize our average winnings.  \n",
        "\n",
        "## State\n",
        "The time steps are $k = \\{0,1,2,3\\}$.   During each time step $x_k$ is the amount of money in the pot which can range from 0 to 3.    We let state $s$ be a function\n",
        "\n",
        "$\n",
        "s = k*4 + x_k\n",
        "$\n",
        "\n",
        "Thus there are 16 states even though some of them are unrealizable in the game (for instance state $s=2$ is $k=0$ and $x_0 = 2$ which is impossible since we can't start the game with 2 dollars in the pot).  \n",
        "## Equations\n",
        "\n",
        "We let $v[(k,x)]$ represent the value of being in state $x$ at turn $k$; that is, with $x$ dollars in the pot at turn $k$.   The value $v[0,0]$ is the expected value we can earn in this game starting at the start state with 0 dollars in the pot.  That is the ultimate value we seek.\n",
        "\n",
        "Let $Q[(k,x),a]$ be the action-value function.  The state we are in is state $(k,x)$ and the action is $a$.   This is the expected value of starting in this state and taking this action.   \n",
        "\n",
        "Let $P[(k,x)]$ be the policy -- the best action choice at state $(k,x)$.   We let 0=pass and 1=flip.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J83ALCN_UCxo"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVtWTos--RdK"
      },
      "source": [
        "## Key Monte Carlo and System Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ffkp-NgLUT_E"
      },
      "outputs": [],
      "source": [
        "\n",
        "#------------------------------------------------------------- check\n",
        "def update (s,a):\n",
        "  '''\n",
        "  Compute next state given state s and action a\n",
        "  s = current state\n",
        "    s%4 = number of dollars in pot\n",
        "    s//4 = turn number k=0,1,2,3\n",
        "  a = 0 means pass\n",
        "  a = 1 means flip\n",
        "  return state, reward (0 for all trajectories)\n",
        "  '''\n",
        "  turn = s // 4\n",
        "  pot = s % 4\n",
        "\n",
        "  if a == 0:\n",
        "    return pot+12, 0\n",
        "\n",
        "  #flip = np.random.randint(1,3)\n",
        "  #if flip == 1:\n",
        "  if np.random.random() < 0.5:\n",
        "    return (turn+1)*4,0      # tails\n",
        "  else:\n",
        "    return pot+1+(turn+1)*4,0    # heads\n",
        "\n",
        "#------------------------------------------------------------- check\n",
        "def e_greedy(s,policy,epsilon):\n",
        "  '''\n",
        "  Implements an e-greedy policy.\n",
        "  With probability epsilon, it returns random action choice\n",
        "  otherwise returns action choice specified by the policy\n",
        "\n",
        "  s = current state\n",
        "  policy = policy function (an array that is indexed by state)\n",
        "  epsilon (0 to 1) a probability of picking exploratory random action\n",
        "  '''\n",
        "  r = np.random.random()\n",
        "  if r > epsilon:\n",
        "    return policy[s]\n",
        "  else:\n",
        "    return np.random.randint(0,2)\n",
        "\n",
        "#------------------------------------------------------------- check\n",
        "def make_trajectory (policy,epsilon):\n",
        "  '''\n",
        "  Simulate one trajectory of experience\n",
        "  Return list of tuples during trajectory\n",
        "  Each tuple is (s,a,r) -> state / action / reward\n",
        "  epsilon = probability of exploratory action\n",
        "  '''\n",
        "  traj = []\n",
        "  k=0\n",
        "  s=0   #  turn 0, pot 0\n",
        "\n",
        "  # s < 12 implies turn k < 3\n",
        "  while (s < 12):\n",
        "    a = e_greedy(s,policy,epsilon)\n",
        "    s_prev = s\n",
        "    s,r = update(s,a)\n",
        "    traj.append((s_prev,a,r))\n",
        "\n",
        "  # final reward = state value, final action = 0 (meaningless)\n",
        "  traj.append((s,0,s%4))\n",
        "  return traj\n",
        "\n",
        "#------------------------------------------------------------- check\n",
        "def init ():\n",
        "  '''\n",
        "  Create totals, counts and policy defaults\n",
        "  '''\n",
        "  totals = np.zeros((16,2))\n",
        "  counts = np.zeros((16,2)).astype(int)\n",
        "  P = np.zeros(16).astype(int)\n",
        "  return totals,counts,P\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_improvement(Q):\n",
        "  '''\n",
        "  Update value function V and policy P based on Q values\n",
        "  '''\n",
        "  V = np.max(Q,axis=1)\n",
        "  P = np.argmax(Q,axis=1)\n",
        "  return V,P\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_evaluation(totals,counts,policy,n,epsilon):\n",
        "  '''\n",
        "  do n trajectories of learning\n",
        "  and update the v/count arrays\n",
        "  '''\n",
        "  for i in range(n):\n",
        "    t = make_trajectory(policy,epsilon)\n",
        "    m = len(t)\n",
        "    #print(m,t)\n",
        "    sum_r = np.zeros(m)\n",
        "    sum_r[m-1] = t[-1][2]\n",
        "    for j in range(m-2,-1,-1):\n",
        "      sum_r[j] = sum_r[j+1] + t[j][2]\n",
        "    #print(sum_r)\n",
        "\n",
        "    for j in range(m):\n",
        "      s,a,r = t[j]\n",
        "      counts[s,a] += 1\n",
        "      totals[s,a] += sum_r[j]\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_iteration(totals,counts,policy,Q,n,m,epsilon):\n",
        "  '''\n",
        "  Perform n iterations of policy iteration\n",
        "  using m trials (episodes) per policy update\n",
        "  '''\n",
        "  for i in range(n):\n",
        "    Q = compute_Q(totals,counts)\n",
        "    V,P = policy_improvement(Q)\n",
        "    policy_evaluation(totals,counts,P,m,epsilon)\n",
        "\n",
        "  print(counts)\n",
        "  Q = compute_Q(totals,counts)\n",
        "  V,P = policy_improvement(Q)\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def compute_Q (totals,counts):\n",
        "  '''\n",
        "  Compute the Q values based on totals and counts (average)\n",
        "  '''\n",
        "  Q = np.zeros((16,2))\n",
        "  for i in range(len(totals)):\n",
        "    for a in range(2):\n",
        "      if counts[i][a] > 0:\n",
        "        Q[i][a] = totals[i][a] / counts[i][a]\n",
        "      else:\n",
        "        Q[i][a] = 0\n",
        "  return Q\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def assess (policy,trials):\n",
        "  '''\n",
        "  Assess the value of the current policy by completing #trials\n",
        "  using the specified policy (no e-greedy random actions)\n",
        "  Does not accrue learning experience nor change policy\n",
        "  '''\n",
        "  totals,counts,P = init()\n",
        "  policy_evaluation(totals,counts,policy,trials,0)\n",
        "  Q = compute_Q(totals,counts)\n",
        "  V,P = policy_improvement(Q)\n",
        "  return V[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihFXVWsB_4jv"
      },
      "source": [
        "### Test Trajectory\n",
        "Run a couple of trajectories and test their accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIAKB5YHXMqO",
        "outputId": "8513bd4d-c1c9-4c43-dac4-fd2f290d157f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, np.int64(0), 0), (12, 0, 0)]\n",
            "[(0, 1, 0), (4, np.int64(0), 0), (12, 0, 0)]\n",
            "[(0, np.int64(0), 0), (12, 0, 0)]\n",
            "[(0, 0, 0), (12, 0, 0)]\n",
            "[(0, 1, 0), (4, np.int64(0), 0), (12, 0, 0)]\n",
            "[(0, 0, 0), (12, 0, 0)]\n",
            "[(0, np.int64(0), 0), (12, 0, 0)]\n",
            "[(0, 0, 0), (12, 0, 0)]\n",
            "[(0, np.int64(0), 0), (12, 0, 0)]\n",
            "[(0, np.int64(0), 0), (12, 0, 0)]\n",
            "Average Game Value: 0.0\n"
          ]
        }
      ],
      "source": [
        "totals,counts,P = init()\n",
        "#np.random.seed(seed=42)\n",
        "epsilon = 0.3\n",
        "print('P =',P)\n",
        "sumt = 0\n",
        "k = 10\n",
        "for i in range(k):\n",
        "  t = make_trajectory(P,epsilon)\n",
        "  sumt += t[-1][-1]\n",
        "  if k <= 10:\n",
        "    print(t)\n",
        "\n",
        "print(\"Average Game Value:\",sumt/k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tESNxCyAVGa"
      },
      "source": [
        "### Do MC Learning\n",
        "This next block does a real segment of MC learning\n",
        "- Start with initial (blank) learning experience\n",
        "- Do 1000 iterations of Policy Iteration each with 100 trials of experience\n",
        "- Extract policy, value function and Q values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCBEhfY6UzqA",
        "outputId": "81242f13-b14f-4365-f8ff-5faac69c69c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 25229 474771]\n",
            " [     0      0]\n",
            " [     0      0]\n",
            " [     0      0]\n",
            " [ 11924 224742]\n",
            " [ 14019 224086]\n",
            " [     0      0]\n",
            " [     0      0]\n",
            " [ 11175 213506]\n",
            " [106519   5812]\n",
            " [106289   5527]\n",
            " [     0      0]\n",
            " [160502      0]\n",
            " [227556      0]\n",
            " [109158      0]\n",
            " [  2784      0]]\n",
            "Q =\n",
            " [[0.         0.95672229]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.73788611]\n",
            " [1.         1.22440045]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.50124118]\n",
            " [1.         0.98726772]\n",
            " [2.         1.51112719]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [1.         0.        ]\n",
            " [2.         0.        ]\n",
            " [3.         0.        ]]\n",
            "V =\n",
            " [0.95672229 0.         0.         0.         0.73788611 1.22440045\n",
            " 0.         0.         0.50124118 1.         2.         0.\n",
            " 0.         1.         2.         3.        ]\n",
            "P =\n",
            " [1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "totals,counts,P = init()\n",
        "m = 100\n",
        "n = 5000\n",
        "epsilon = 0.1\n",
        "Q = compute_Q(totals,counts)\n",
        "V,P = policy_improvement(Q)\n",
        "policy_iteration(totals,counts,P,Q,n,m,epsilon)\n",
        "\n",
        "Q = compute_Q(totals,counts)\n",
        "V,P = policy_improvement(Q)\n",
        "print('Q =\\n',Q)\n",
        "print('V =\\n',V)\n",
        "print('P =\\n',P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF4FMqiXffPY"
      },
      "source": [
        "### Pretty Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYJCyhyFfhhV",
        "outputId": "b993fc06-99ae-48bc-a999-156e01528537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Turn [0]  Pot 0   |  Policy: flip   Value: 0.96\n",
            "\n",
            "Turn [1]  Pot 0   |  Policy: flip   Value: 0.74\n",
            "Turn [1]  Pot 1   |  Policy: flip   Value: 1.22\n",
            "\n",
            "Turn [2]  Pot 0   |  Policy: flip   Value: 0.50\n",
            "Turn [2]  Pot 1   |  Policy: stay   Value: 1.00\n",
            "Turn [2]  Pot 2   |  Policy: stay   Value: 2.00\n",
            "\n",
            "Turn [3]  Pot 0   |  Policy: stay   Value: 0.00\n",
            "Turn [3]  Pot 1   |  Policy: stay   Value: 1.00\n",
            "Turn [3]  Pot 2   |  Policy: stay   Value: 2.00\n",
            "Turn [3]  Pot 3   |  Policy: stay   Value: 3.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for k in range(4):\n",
        "  for pot in range(k+1):\n",
        "    s = k*4 + pot\n",
        "    print(\"Turn [{0:d}]  Pot {3:d}   |  Policy: {1:4s}   Value: {2:4.2f}\".format(k,\"stay\" if P[s]==0 else \"flip\",V[s],pot))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWyqcli6ApHY"
      },
      "source": [
        "### Assessment\n",
        "We can also assess the current policy by conduction many non-learning trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pBoKmlE9WFm5",
        "outputId": "90f25b7d-aaf6-48a0-9414-90048a8aa918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0025\n"
          ]
        }
      ],
      "source": [
        "value = assess(P,2000)\n",
        "print(value)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
